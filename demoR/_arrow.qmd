---
title: "Doing More with Bigger Data: An Introduction to Arrow for R Users"
format:
  aps-html:
    code-tools: true
---

## Setup

```{r}
#| label: setup
#| message: false
library(tidyverse)
#install.packages('arrow', repos = c('https://apache.r-universe.dev'))
library(arrow)
library(dbplyr, warn.conflicts = FALSE)
library(duckdb)
```

## Getting the data

We use a dataset of item checkouts from Seattle public libraries, available online at [Seattle](https://data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6/about_data). 

The following code will get you a cached copy of the data. The data is quite big, so it will take some time to download. I highly recommend using curl::multi_download() to get very large files as itâ€™s built for exactly this purpose: it gives you a progress bar and it can resume the download if its interrupted.

```{r}
#| eval: false
#| include: false
dir.create("data", showWarnings = FALSE)

curl::multi_download(
  "https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv",
  "data/seattle-library-checkouts.csv",
  resume = TRUE
)
```



## Reading the data

If we attempt to read the dataset using `read_csv()`, it could result in a lengthy processing time or potentially fail altogether. This is primarily due to the sheer size of the dataset, which contains approximately 31871534 rows, 12 columns, and occupies 6.5 GB of storage space.
```{r}
#| eval: false
#| include: false
read_csv("data/seattle-library-checkouts.csv") |> 
  nrow()
```

```{r}
seattle_csv <- open_dataset(
  sources = "data/seattle-library-checkouts.csv", 
  col_types = schema(ISBN = string()),
  format = "csv"
)
seattle_csv 
```


