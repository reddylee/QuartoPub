{
  "hash": "7c2b95000a063b2dc32e8c773fd82133",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Doing More with Bigger Data: An Introduction to Arrow for R Users\"\nformat:\n  aps-html:\n    number-sections: true\n    code-tools: true\n---\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n#install.packages('arrow', repos = c('https://apache.r-universe.dev'))\nlibrary(arrow)\nlibrary(dbplyr, warn.conflicts = FALSE)\nlibrary(duckdb)\nlibrary(lobstr)\nlibrary(tictoc)\ndatapath <- \"~/Github/data\"\n```\n:::\n\n\n## Getting the data\n\nWe use a dataset of item checkouts from Seattle public libraries, available online at [Seattle](https://data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6/about_data). \n\nThe following code will get you a cached copy of the data. The data is quite big, so it will take some time to download. I highly recommend using `curl::multi_download()` to get very large files as it’s built for exactly this purpose: it gives you a progress bar and it can resume the download if its interrupted.\n\n```{{r}}\ndir.create(\"data\", showWarnings = FALSE)\n\ncurl::multi_download(\n  \"https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv\",\n  \"~/Github/data/seattle-library-checkouts.csv\",\n  resume = TRUE\n)\n```\n\n## Reading the data as a CSV file\n\nIf we attempt to read the dataset using `read_csv()`, it could result in a lengthy processing time or potentially fail altogether (In my case, I have to force quit R due to no responding). This is primarily due to the sheer size of the dataset, which contains 41,389,465 rows, 12 columns, and occupies 9.21 GB of storage space.\n```{{r}}\n#| eval: false\n#| include: false\nread_csv(\"~/Github/data/seattle-library-checkouts.csv\") |> \n  nrow()\n```\n\nLet's use the `open_dataset()` function from the `arrow` package to read the dataset. This function is designed to read large datasets efficiently by only reading the metadata and not the entire dataset. This allows us to work with large datasets without running into memory issues.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv <- open_dataset(\n  sources = \"~/GitHub/data/seattle-library-checkouts.csv\", \n  col_types = schema(ISBN = string()),\n  format = \"csv\"\n)\nseattle_csv \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFileSystemDataset with 1 csv file\nUsageClass: string\nCheckoutType: string\nMaterialType: string\nCheckoutYear: int64\nCheckoutMonth: int64\nCheckouts: int64\nTitle: string\nISBN: string\nCreator: string\nSubjects: string\nPublisher: string\nPublicationYear: string\n```\n\n\n:::\n:::\n\n\nEven so, it still took a much longer time (about 35 seconds) to get the rows, which is 41,389,465 rows.\n```{{r}}\ntic()\nseattle_csv |> \n  count() |> \n  collect()\ntoc()\n```\n\nTake a glimpse at the dataset also took a long time (about 20 seconds).\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nseattle_csv |> glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFileSystemDataset with 1 csv file\n41,389,465 rows x 12 columns\n$ UsageClass      <string> \"Physical\", \"Physical\", \"Digital\", \"Physical\", \"Physi…\n$ CheckoutType    <string> \"Horizon\", \"Horizon\", \"OverDrive\", \"Horizon\", \"Horizo…\n$ MaterialType    <string> \"BOOK\", \"BOOK\", \"EBOOK\", \"BOOK\", \"SOUNDDISC\", \"BOOK\",…\n$ CheckoutYear     <int64> 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016,…\n$ CheckoutMonth    <int64> 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n$ Checkouts        <int64> 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 3, 2, 1, 3, 2, 3,…\n$ Title           <string> \"Super rich : a guide to having it all / Russell Simm…\n$ ISBN            <string> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ Creator         <string> \"Simmons, Russell\", \"Barclay, James, 1965-\", \"Tim Par…\n$ Subjects        <string> \"Self realization, Conduct of life, Attitude Psycholo…\n$ Publisher       <string> \"Gotham Books,\", \"Pyr,\", \"Random House, Inc.\", \"Dial …\n$ PublicationYear <string> \"c2011.\", \"2010.\", \"2015\", \"2005.\", \"c2004.\", \"c2005.…\n```\n\n\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n14.821 sec elapsed\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nseattle_csv |> \n  summarise(Checkouts = sum(Checkouts),\n            .by = CheckoutYear) |> \n  arrange(CheckoutYear) |> \n  collect()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"CheckoutYear\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Checkouts\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2005\",\"2\":\"3798685\"},{\"1\":\"2006\",\"2\":\"6599318\"},{\"1\":\"2007\",\"2\":\"7126627\"},{\"1\":\"2008\",\"2\":\"8438486\"},{\"1\":\"2009\",\"2\":\"9135167\"},{\"1\":\"2010\",\"2\":\"8608966\"},{\"1\":\"2011\",\"2\":\"8321732\"},{\"1\":\"2012\",\"2\":\"8163046\"},{\"1\":\"2013\",\"2\":\"9057096\"},{\"1\":\"2014\",\"2\":\"9136081\"},{\"1\":\"2015\",\"2\":\"9084179\"},{\"1\":\"2016\",\"2\":\"9021051\"},{\"1\":\"2017\",\"2\":\"9231648\"},{\"1\":\"2018\",\"2\":\"9149176\"},{\"1\":\"2019\",\"2\":\"9199083\"},{\"1\":\"2020\",\"2\":\"6053717\"},{\"1\":\"2021\",\"2\":\"7361031\"},{\"1\":\"2022\",\"2\":\"7001989\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n19.458 sec elapsed\n```\n\n\n:::\n:::\n\n\nAnd it took about 20 seconds to count the **Checkouts** by year.\n\n## Reading the data as a Partquet file\nThanks to arrow, this code will work regardless of how large the underlying dataset is. But it’s currently rather slow: on mycomputer, it took 20 seconds or longer to run. That’s not terrible given how much data we have, but we can make it much faster by switching to a better format.\n\n### Rewriting the Seattle library data as a Parquet file\n\n```{{r}}\nseattle_csv |>\n  group_by(CheckoutYear) |>\n  write_dataset(path = \"~/GitHub/data/seattle-library-checkouts\", format = \"parquet\")\n```\n\nIt took about 30 seconds to write the dataset as a Parquet file, which is 4,42 GB in size, less than half the size of the original CSV file. Let’s take a look at what we just produced:\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  files = list.files(\"~/GitHub/data/seattle-library-checkouts\", recursive = TRUE),\n  size_MB = file.size(file.path(\"~/GitHub/data/seattle-library-checkouts\", files)) / 1024^2\n)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"files\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"size_MB\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"CheckoutYear=2005/part-0.parquet\",\"2\":\"109.1460\"},{\"1\":\"CheckoutYear=2006/part-0.parquet\",\"2\":\"163.8052\"},{\"1\":\"CheckoutYear=2007/part-0.parquet\",\"2\":\"177.5494\"},{\"1\":\"CheckoutYear=2008/part-0.parquet\",\"2\":\"194.6294\"},{\"1\":\"CheckoutYear=2009/part-0.parquet\",\"2\":\"213.9811\"},{\"1\":\"CheckoutYear=2010/part-0.parquet\",\"2\":\"222.1355\"},{\"1\":\"CheckoutYear=2011/part-0.parquet\",\"2\":\"238.5952\"},{\"1\":\"CheckoutYear=2012/part-0.parquet\",\"2\":\"248.7223\"},{\"1\":\"CheckoutYear=2013/part-0.parquet\",\"2\":\"268.8119\"},{\"1\":\"CheckoutYear=2014/part-0.parquet\",\"2\":\"281.9293\"},{\"1\":\"CheckoutYear=2015/part-0.parquet\",\"2\":\"293.5520\"},{\"1\":\"CheckoutYear=2016/part-0.parquet\",\"2\":\"300.2249\"},{\"1\":\"CheckoutYear=2017/part-0.parquet\",\"2\":\"304.1663\"},{\"1\":\"CheckoutYear=2018/part-0.parquet\",\"2\":\"292.0176\"},{\"1\":\"CheckoutYear=2019/part-0.parquet\",\"2\":\"288.4465\"},{\"1\":\"CheckoutYear=2020/part-0.parquet\",\"2\":\"150.8536\"},{\"1\":\"CheckoutYear=2021/part-0.parquet\",\"2\":\"228.7637\"},{\"1\":\"CheckoutYear=2022/part-0.parquet\",\"2\":\"240.5243\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nOur single 9GB CSV file has been rewritten into 18 parquet files. The file names use a “self-describing” convention used by the Apache Hive project. Hive-style partitions name folders with a “key=value” convention, so as you might guess, the CheckoutYear=2005 directory contains all the data where CheckoutYear is 2005. Each file is between 100 and 300 MB and the total size is now around 4 GB, a little over half the size of the original CSV file. This is as we expect since parquet is a much more efficient format.\n\n### Using dplyr with arrow\nNow we’ve created these parquet files, we’ll need to read them in again. We use open_dataset() again, but this time we give it a directory:\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_pq <- open_dataset(\"~/GitHub/data/seattle-library-checkouts\")\n```\n:::\n\nNow we can write our dplyr pipeline. For example, we could count the total number of books checked out in each month for the last five years:\n\n::: {.cell}\n\n```{.r .cell-code}\nquery <- seattle_pq |> \n  filter(CheckoutYear >= 2018, MaterialType == \"BOOK\") |>\n  group_by(CheckoutYear, CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(CheckoutYear, CheckoutMonth)\n```\n:::\n\nWriting dplyr code for arrow data is conceptually similar to dbplyr, Chapter 21: you write dplyr code, which is automatically transformed into a query that the Apache Arrow C++ library understands, which is then executed when you call collect(). If we print out the query object we can see a little information about what we expect Arrow to return when the execution takes place. And we can get the results by calling collect():\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nFileSystemDataset (query)\nCheckoutYear: int32\nCheckoutMonth: int64\nTotalCheckouts: int64\n\n* Grouped by CheckoutYear\n* Sorted by CheckoutYear [asc], CheckoutMonth [asc]\nSee $.data for the source Arrow object\n```\n\n\n:::\n\n```{.r .cell-code}\nquery |> \n  collect()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"CheckoutYear\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"CheckoutMonth\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"TotalCheckouts\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2018\",\"2\":\"1\",\"3\":\"355101\"},{\"1\":\"2018\",\"2\":\"2\",\"3\":\"309813\"},{\"1\":\"2018\",\"2\":\"3\",\"3\":\"344487\"},{\"1\":\"2018\",\"2\":\"4\",\"3\":\"330988\"},{\"1\":\"2018\",\"2\":\"5\",\"3\":\"318049\"},{\"1\":\"2018\",\"2\":\"6\",\"3\":\"341825\"},{\"1\":\"2018\",\"2\":\"7\",\"3\":\"351207\"},{\"1\":\"2018\",\"2\":\"8\",\"3\":\"352977\"},{\"1\":\"2018\",\"2\":\"9\",\"3\":\"319587\"},{\"1\":\"2018\",\"2\":\"10\",\"3\":\"338497\"},{\"1\":\"2018\",\"2\":\"11\",\"3\":\"325249\"},{\"1\":\"2018\",\"2\":\"12\",\"3\":\"299789\"},{\"1\":\"2019\",\"2\":\"1\",\"3\":\"352288\"},{\"1\":\"2019\",\"2\":\"2\",\"3\":\"299465\"},{\"1\":\"2019\",\"2\":\"3\",\"3\":\"326457\"},{\"1\":\"2019\",\"2\":\"4\",\"3\":\"326988\"},{\"1\":\"2019\",\"2\":\"5\",\"3\":\"314565\"},{\"1\":\"2019\",\"2\":\"6\",\"3\":\"331914\"},{\"1\":\"2019\",\"2\":\"7\",\"3\":\"356688\"},{\"1\":\"2019\",\"2\":\"8\",\"3\":\"340723\"},{\"1\":\"2019\",\"2\":\"9\",\"3\":\"328773\"},{\"1\":\"2019\",\"2\":\"10\",\"3\":\"334295\"},{\"1\":\"2019\",\"2\":\"11\",\"3\":\"323796\"},{\"1\":\"2019\",\"2\":\"12\",\"3\":\"295736\"},{\"1\":\"2020\",\"2\":\"1\",\"3\":\"343328\"},{\"1\":\"2020\",\"2\":\"2\",\"3\":\"317946\"},{\"1\":\"2020\",\"2\":\"3\",\"3\":\"215128\"},{\"1\":\"2020\",\"2\":\"4\",\"3\":\"70\"},{\"1\":\"2020\",\"2\":\"5\",\"3\":\"87\"},{\"1\":\"2020\",\"2\":\"6\",\"3\":\"43\"},{\"1\":\"2020\",\"2\":\"7\",\"3\":\"52\"},{\"1\":\"2020\",\"2\":\"8\",\"3\":\"8276\"},{\"1\":\"2020\",\"2\":\"9\",\"3\":\"65204\"},{\"1\":\"2020\",\"2\":\"10\",\"3\":\"91168\"},{\"1\":\"2020\",\"2\":\"11\",\"3\":\"91740\"},{\"1\":\"2020\",\"2\":\"12\",\"3\":\"108957\"},{\"1\":\"2021\",\"2\":\"1\",\"3\":\"128967\"},{\"1\":\"2021\",\"2\":\"2\",\"3\":\"129312\"},{\"1\":\"2021\",\"2\":\"3\",\"3\":\"152856\"},{\"1\":\"2021\",\"2\":\"4\",\"3\":\"143800\"},{\"1\":\"2021\",\"2\":\"5\",\"3\":\"148170\"},{\"1\":\"2021\",\"2\":\"6\",\"3\":\"185533\"},{\"1\":\"2021\",\"2\":\"7\",\"3\":\"224132\"},{\"1\":\"2021\",\"2\":\"8\",\"3\":\"229374\"},{\"1\":\"2021\",\"2\":\"9\",\"3\":\"241336\"},{\"1\":\"2021\",\"2\":\"10\",\"3\":\"244967\"},{\"1\":\"2021\",\"2\":\"11\",\"3\":\"245485\"},{\"1\":\"2021\",\"2\":\"12\",\"3\":\"192506\"},{\"1\":\"2022\",\"2\":\"1\",\"3\":\"261763\"},{\"1\":\"2022\",\"2\":\"2\",\"3\":\"222464\"},{\"1\":\"2022\",\"2\":\"3\",\"3\":\"251502\"},{\"1\":\"2022\",\"2\":\"4\",\"3\":\"236443\"},{\"1\":\"2022\",\"2\":\"5\",\"3\":\"240284\"},{\"1\":\"2022\",\"2\":\"6\",\"3\":\"249086\"},{\"1\":\"2022\",\"2\":\"7\",\"3\":\"232558\"},{\"1\":\"2022\",\"2\":\"8\",\"3\":\"257218\"},{\"1\":\"2022\",\"2\":\"9\",\"3\":\"239864\"},{\"1\":\"2022\",\"2\":\"10\",\"3\":\"240320\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nLet's compare the time it took to count the **Checkouts** by year using the CSV file and the Parquet file and see if it worth the trouble.\n\n::: {.cell}\n\n```{.r .cell-code}\ncsvtime <- seattle_csv |> \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n  group_by(CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect() |> \n  system.time()\ncsvtime\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n 20.009   3.322  20.048 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npqtime <- seattle_pq |> \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n  group_by(CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect() |> \n  system.time()\npqtime\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.449   0.071   0.131 \n```\n\n\n:::\n:::\n\nParquet file took 0.131 seconds while csv file took 20.048 seconds. The Parquet file is extremely faster, about 100 times, than the CSV file. Totally worth the trouble. \n\n## Using duckdb with arrow\nThere’s one last advantage of parquet and arrow — it’s very easy to turn an arrow dataset into a `DuckDB` database by calling `arrow::to_duckdb()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nseattle_pq |> \n  to_duckdb() |>\n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n  group_by(CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect() \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Missing values are always removed in SQL aggregation functions.\nUse `na.rm = TRUE` to silence this warning\nThis warning is displayed once every 8 hours.\n```\n\n\n:::\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"CheckoutMonth\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"TotalCheckouts\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"12\",\"2\":\"192506\"},{\"1\":\"11\",\"2\":\"245485\"},{\"1\":\"10\",\"2\":\"244967\"},{\"1\":\"9\",\"2\":\"241336\"},{\"1\":\"8\",\"2\":\"229374\"},{\"1\":\"7\",\"2\":\"224132\"},{\"1\":\"6\",\"2\":\"185533\"},{\"1\":\"5\",\"2\":\"148170\"},{\"1\":\"4\",\"2\":\"143800\"},{\"1\":\"3\",\"2\":\"152856\"},{\"1\":\"2\",\"2\":\"129312\"},{\"1\":\"1\",\"2\":\"128967\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.361 sec elapsed\n```\n\n\n:::\n:::\n\nIt took a little longer than without transition. However, the neat thing about `to_duckdb()` is that the transfer doesn’t involve any memory copying, and speaks to the goals of the arrow ecosystem: enabling seamless transitions from one computing environment to another.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}